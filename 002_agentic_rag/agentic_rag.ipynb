{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG Playground by LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from src.tools import GuestInfoRetrieverTool\n",
    "from src.tools import DDGSearchTool\n",
    "from src.tools import WeatherInfoTool\n",
    "from src.tools import HubStatsTool\n",
    "\n",
    "from src.retriever import query_guest_agent\n",
    "from src.retriever import query_multi_step_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        text=\"\\n\".join([\n",
    "            f\"Name: {guest_dataset['name'][i]}\",\n",
    "            f\"Relation: {guest_dataset['relation'][i]}\",\n",
    "            f\"Description: {guest_dataset['description'][i]}\",\n",
    "            f\"Email: {guest_dataset['email'][i]}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest_dataset['name'][i]}\n",
    "    )\n",
    "    for i in range(len(guest_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tool\n",
    "gitr = GuestInfoRetrieverTool(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "question = \"Tell me about our guest named 'Lady Ada Lovelace'.\"\n",
    "\n",
    "# Call the function using default parameters\n",
    "response = await query_guest_agent(docs, question)\n",
    "\n",
    "# Generalize the response handling (same as before)\n",
    "if response and response.response:\n",
    "    print(\"ðŸŽ© Alfred's Response:\")\n",
    "    print(response.response)\n",
    "else:\n",
    "    print(\"ðŸŽ© Alfred did not provide a response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example query\n",
    "question = \"Who is Charles Babbage?\"\n",
    "\n",
    "# Call the function with custom parameters\n",
    "response = await query_guest_agent(\n",
    "    docs,\n",
    "    question,\n",
    "    llm_model=\"phi3:mini\", # Use a different LLM model\n",
    "    system_prompt=\"You are a helpful AI assistant that provides information about people.\" # Use a different system prompt\n",
    ")\n",
    "\n",
    "# Generalize the response handling (same as before)\n",
    "if response and response.response:\n",
    "    print(\"ðŸŽ© Alfred's Response:\")\n",
    "    print(response.response)\n",
    "else:\n",
    "    print(\"ðŸŽ© Alfred did not provide a response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give Your Agent Access to the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DDGSearchTool class\n",
    "ddst = DDGSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Founder and first president of the Turkish Republic. Although AtatÃ¼rk (\"Father of Turks\") left this world behind in 1938, in Turkey his movie hero profile, icy-blue stare, elegant silhouette, and classically tailored suits remain everywhere to be seen. His portrait adorns the walls of teahouses as far flung as Van and Gaziantep.\n"
     ]
    }
   ],
   "source": [
    "# Use the search_tool method of the instance\n",
    "response = ddst.search_tool(\"Who's the founder of Turkish Republic?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tool\n",
    "# The DuckDuckGoSearchTool class already initializes the FunctionTool internally.\n",
    "# We just need to create an instance of the class.\n",
    "wit = WeatherInfoTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hst = HubStatsTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/esmfold_v1 with 20,947,347 downloads.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "response = hst.get_hub_stats(\"facebook\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate weather_info_tool and hub_stats_tool to Alfred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proper FunctionTool using the provided arguments\n",
    "\n",
    "search_tool = FunctionTool.from_defaults(\n",
    "        fn=ddst.search_tool,\n",
    "        name=\"dd_search_tool\",\n",
    "        description=(\n",
    "            \"Use this tool to search the internet for general information about a topic, \"\n",
    "            \"definitions, explanations, or factual details that are likely to be found on websites. \"\n",
    "            \"This is a general-purpose search tool.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "weather_info_tool = FunctionTool.from_defaults(\n",
    "        fn=wit.get_weather_info,\n",
    "        name=\"weather_info_tool\",\n",
    "        description=\"Use this tool ONLY when the user is asking for weather information for a specific location.\"\n",
    "    )\n",
    "\n",
    "hub_stats_tool = FunctionTool.from_defaults(\n",
    "    fn=hst.get_hub_stats,\n",
    "    name=\"hub_stats_tool\",\n",
    "    description=(\n",
    "        \"Use this tool *only* to find statistics about models on the Hugging Face Hub. \"\n",
    "        \"Specifically, use this tool to find the most downloaded model for a given author or organization *registered on the Hugging Face Hub*. \"\n",
    "        \"Input should be the exact author or organization name (e.g., 'facebook', 'google', 'microsoft'). \"\n",
    "        \"Example queries this tool can answer: 'What is the most downloaded model by google on Hugging Face?', 'Tell me about the stats for microsoft on the Hub.'\"\n",
    "    )\n",
    ")\n",
    "\n",
    "guest_info_tool = FunctionTool.from_defaults(\n",
    "    fn=gitr.get_guest_info,\n",
    "    name=\"guest_info_tool\",\n",
    "    description=\"Retrieve comprehensive information about guests attending the gala by their name, including their relation, description, and contact details like email.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is running and ready.\n",
      "Ollama server is already running.\n",
      "Error: Could not find JSON block in LLM output.\n",
      "LLM output: Here's the plan to answer the user query:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"task\": \"Search for general information about Facebook\",\n",
      "    \"tool\": \"dd_search_tool\",\n",
      "    \"tool_input\": {\"query\": \"What is Facebook\"}\n",
      "  },\n",
      "  {\n",
      "    \"task\": \"Find Hugging Face author associated with Facebook's most popular model\",\n",
      "    \"tool\": \"hub_stats_tool\",\n",
      "    \"tool_input\": {\"author\": \"Facebook\"} // Assuming Facebook has a Hugging Face author account\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "In the first step, we use the `dd_search_tool` to search for general information about Facebook. This tool is suitable for finding general information on a topic.\n",
      "\n",
      "In the second step, we assume that Facebook has a Hugging Face author account and uses the `hub_stats_tool` to find their most popular model. Since Facebook is not a typical machine learning model author, this assumption may not hold true in reality. If Facebook is indeed a Hugging Face author, then this tool can provide information on their most popular models.\n",
      "\n",
      "Note that if we cannot assume Facebook has a Hugging Face author account, the second step would need to be re-evaluated or an alternative approach taken.\n",
      "Error: Could not find the planning response in the expected format.\n",
      "ðŸŽ© Alfred's Response:\n",
      "Error: Could not find the planning response in the expected format.\n"
     ]
    }
   ],
   "source": [
    "llm_model=\"llama3:latest\"\n",
    "question = \"What is Facebook and what's their most popular model?\"\n",
    "tools_dict = {\n",
    "     \"dd_search_tool\": search_tool,\n",
    "     \"hub_stats_tool\": hub_stats_tool,\n",
    "     \"weather_info_tool\": weather_info_tool\n",
    " }\n",
    "\n",
    "# To print only the final answer:\n",
    "response = await query_multi_step_agent(question, tools_dict, llm_model, print_details=False)\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your Gala Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is running and ready.\n",
      "Ollama server is already running.\n",
      "ðŸŽ© Alfred's Response:\n",
      "Lady Ada Lovelace was the daughter of Lord Byron, but her mother, Anne Isabella Milbanke, encouraged her interest in mathematics and science to counterbalance her father's artistic inclinations. She became fascinated with Charles Babbage's proposed analytic engine, seeing its potential for programming and calculation. Her education was largely self-directed, with guidance from Mary Somerville, a Scottish mathematician and astronomer. Lovelace's contributions to mathematics and computing lie in her notes on the analytic engine, where she described how it could be programmed and predicted its capabilities, making her often regarded as the first computer programmer.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about Lady Ada Lovelace. What's her background?\"\n",
    "llm_model = \"llama3:latest\"\n",
    "tools_dict = {\n",
    "     \"dd_search_tool\": search_tool,\n",
    "     \"hub_stats_tool\": hub_stats_tool,\n",
    "     \"weather_info_tool\": weather_info_tool,\n",
    "     \"guest_info_tool\": guest_info_tool\n",
    " }\n",
    "\n",
    "# To print only the final answer:\n",
    "response = await query_multi_step_agent(query, tools_dict, llm_model, print_details=False)\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is running and ready.\n",
      "Ollama server is already running.\n",
      "ðŸŽ© Alfred's Response:\n",
      "The weather in Paris tonight is rainy with a temperature of 15Â°C. Itâ€™s not suitable for your fireworks display.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\"\n",
    "llm_model = \"gemma3:1b\"\n",
    "tools_dict = {\n",
    "     \"dd_search_tool\": search_tool,\n",
    "     \"hub_stats_tool\": hub_stats_tool,\n",
    "     \"weather_info_tool\": weather_info_tool,\n",
    "     \"guest_info_tool\": guest_info_tool\n",
    " }\n",
    "\n",
    "# To print only the final answer:\n",
    "response = await query_multi_step_agent(query, tools_dict, llm_model, print_details=False)\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is running and ready.\n",
      "Ollama server is already running.\n",
      "ðŸŽ© Alfred's Response:\n",
      "The most downloaded model by Google is google/electra-base-discriminator, with 17,910,835 downloads.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"One of our guests is from Google. What can you tell me about their most popular model?\"\n",
    "llm_model = \"gemma2:2b\"\n",
    "tools_dict = {\n",
    "     \"dd_search_tool\": search_tool,\n",
    "     \"hub_stats_tool\": hub_stats_tool,\n",
    "     \"weather_info_tool\": weather_info_tool,\n",
    "     \"guest_info_tool\": guest_info_tool\n",
    " }\n",
    "\n",
    "# To print only the final answer:\n",
    "response = await query_multi_step_agent(query, tools_dict, llm_model, print_details=False)\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server is running and ready.\n",
      "Ollama server is already running.\n",
      "ðŸŽ© Alfred's Response:\n",
      "To prepare for your conversation with Dr. Nikola Tesla about recent advancements in wireless energy, it's essential to familiarize yourself with the two primary methods: inductive wireless power transfer and magnetic resonant coupling WPT. Both methods utilize magnetic fields as the medium for energy transfer. In the case of magnetic resonant coupling WPT, studies initially didn't incorporate compensation networks to enhance system performance, relying solely on magnetic induction between two coils.\n"
     ]
    }
   ],
   "source": [
    "query = \"I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\"\n",
    "llm_model = \"llama3:latest\"\n",
    "tools_dict = {\n",
    "     \"dd_search_tool\": search_tool,\n",
    "     \"hub_stats_tool\": hub_stats_tool,\n",
    "     \"weather_info_tool\": weather_info_tool,\n",
    "     \"guest_info_tool\": guest_info_tool\n",
    " }\n",
    "\n",
    "# To print only the final answer:\n",
    "response = await query_multi_step_agent(query, tools_dict, llm_model, print_details=False)\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
