{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG Playground by LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "from src.tools import GuestInfoRetrieverTool\n",
    "from src.retriever import query_guest_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        text=\"\\n\".join([\n",
    "            f\"Name: {guest_dataset['name'][i]}\",\n",
    "            f\"Relation: {guest_dataset['relation'][i]}\",\n",
    "            f\"Description: {guest_dataset['description'][i]}\",\n",
    "            f\"Email: {guest_dataset['email'][i]}\"\n",
    "        ]),\n",
    "        metadata={\"name\": guest_dataset['name'][i]}\n",
    "    )\n",
    "    for i in range(len(guest_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tool\n",
    "guest_info_tool = GuestInfoRetrieverTool(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "question = \"Tell me about our guest named 'Lady Ada Lovelace'.\"\n",
    "\n",
    "# Call the function using default parameters\n",
    "response = await query_guest_agent(docs, question)\n",
    "\n",
    "# Generalize the response handling (same as before)\n",
    "if response and response.response:\n",
    "    print(\"ðŸŽ© Alfred's Response:\")\n",
    "    print(response.response)\n",
    "else:\n",
    "    print(\"ðŸŽ© Alfred did not provide a response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example query\n",
    "question = \"Who is Charles Babbage?\"\n",
    "\n",
    "# Call the function with custom parameters\n",
    "response = await query_guest_agent(\n",
    "    docs,\n",
    "    question,\n",
    "    llm_model=\"phi3:mini\", # Use a different LLM model\n",
    "    system_prompt=\"You are a helpful AI assistant that provides information about people.\" # Use a different system prompt\n",
    ")\n",
    "\n",
    "# Generalize the response handling (same as before)\n",
    "if response and response.response:\n",
    "    print(\"ðŸŽ© Alfred's Response:\")\n",
    "    print(response.response)\n",
    "else:\n",
    "    print(\"ðŸŽ© Alfred did not provide a response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give Your Agent Access to the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import DDGSearchTool\n",
    "\n",
    "# Create an instance of the DDGSearchTool class\n",
    "ddst = DDGSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the search_tool method of the instance\n",
    "response = ddst.search_tool(\"Who's the founder of Turkish Republic?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import WeatherInfoTool\n",
    "\n",
    "# Initialize the tool\n",
    "# The DuckDuckGoSearchTool class already initializes the FunctionTool internally.\n",
    "# We just need to create an instance of the class.\n",
    "wit = WeatherInfoTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools import HubStatsTool\n",
    "\n",
    "hst = HubStatsTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "response = hst.get_hub_stats(\"facebook\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate weather_info_tool and hub_stats_tool to Alfred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generalized_multi_step_agent(query, llm, tools):\n",
    "    # Step 1: Plan the execution\n",
    "    planning_prompt = f\"\"\"\n",
    "    Analyze the following user query and create a plan to answer it using the available tools.\n",
    "    Available tools:\n",
    "    - dd_search_tool: Use to search the internet for general information.\n",
    "    - hub_stats_tool: Use to find the most downloaded model for a Hugging Face author.\n",
    "    - weather_info_tool: Use to get weather information for a location.\n",
    "\n",
    "    Output the plan as a JSON array of steps. Each step should have:\n",
    "    - \"task\": A description of the task for this step.\n",
    "    - \"tool\": The name of the tool to use (from the available tools).\n",
    "    - \"tool_input\": The input for the tool (e.g., a search query, an author name, a location).\n",
    "\n",
    "    Example output format:\n",
    "    ```json\n",
    "    [\n",
    "      {{\n",
    "        \"task\": \"Example task\",\n",
    "        \"tool\": \"example_tool\",\n",
    "        \"tool_input\": {{\"key\": \"value\"}}\n",
    "      }}\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    User query: {query}\n",
    "    \"\"\"\n",
    "    plan_response = await llm.acomplete(planning_prompt)\n",
    "\n",
    "    # Extract the JSON plan from the LLM's output\n",
    "    plan_text = plan_response.text.strip()\n",
    "    json_start = plan_text.find(\"```json\")\n",
    "    json_end = plan_text.find(\"```\", json_start + 7) # Find the closing ``` after the opening one\n",
    "\n",
    "    if json_start != -1 and json_end != -1:\n",
    "        # Extract the text between the triple backticks\n",
    "        json_string = plan_text[json_start + 7:json_end].strip()\n",
    "        try:\n",
    "            plan = json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON plan: {e}\")\n",
    "            print(f\"Extracted JSON string: {json_string}\")\n",
    "            return \"Error: Could not parse the planning response from the LLM.\"\n",
    "    else:\n",
    "        print(\"Error: Could not find JSON block in LLM output.\")\n",
    "        print(f\"LLM output: {plan_text}\")\n",
    "        return \"Error: Could not find the planning response in the expected format.\"\n",
    "\n",
    "    tool_results = {}\n",
    "    \n",
    "    # Step 2: Execute the plan\n",
    "    for step in plan:\n",
    "        task = step[\"task\"]\n",
    "        tool_name = step[\"tool\"]\n",
    "        tool_input = step[\"tool_input\"]\n",
    "\n",
    "        if tool_name in tools:\n",
    "            tool_instance = tools[tool_name]\n",
    "            try:\n",
    "                result = await tool_instance.acall(**tool_input) # Use **tool_input to unpack dict\n",
    "                # Extract the text content from the ToolOutput object\n",
    "                tool_results[task] = result.content\n",
    "                # print(f\"Executed '{task}': {result.content}\") # Comment out or remove this line\n",
    "            except Exception as e:\n",
    "                tool_results[task] = f\"Error executing tool {tool_name}: {str(e)}\\n\" # Added newline for better formatting\n",
    "                # print(f\"Error executing '{task}': {e}\") # Comment out or remove this linets[task] = result.content\n",
    "                print(f\"Executed '{task}': {result.content}\") # This line prints the execution details\n",
    "            except Exception as e:\n",
    "                tool_results[task] = f\"Error executing tool {tool_name}: {str(e)}\\n\" # Added newline for better formatting\n",
    "                print(f\"Error executing '{task}': {e}\") \n",
    "        else:\n",
    "            tool_results[task] = f\"Error: Tool '{tool_name}' not found.\"\n",
    "            print(f\"Error: Tool '{tool_name}' not found.\")\n",
    "\n",
    "    # Step 3: Synthesize the information\n",
    "    synthesis_prompt = f\"\"\"\n",
    "    The user asked: '{query}'.\n",
    "    You gathered the following information:\n",
    "    {json.dumps(tool_results, indent=2)}\n",
    "\n",
    "    Please synthesize this information into a comprehensive answer that addresses all parts of the original question.\n",
    "    \"\"\"\n",
    "    synthesized_answer = await llm.acomplete(synthesis_prompt)\n",
    "\n",
    "    # Return both the tool results and the synthesized answer\n",
    "    return {\n",
    "        \"tool_results\": tool_results,\n",
    "        \"final_answer\": synthesized_answer.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import traceback\n",
    "from llama_index.core.tools import FunctionTool\n",
    "import asyncio\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "llm = Ollama(model=\"llama3:latest\", request_timeout=1200)\n",
    "\n",
    "search_tool = FunctionTool.from_defaults(\n",
    "        fn=ddst.search_tool,\n",
    "        name=\"dd_search_tool\",\n",
    "        description=(\n",
    "            \"Use this tool to search the internet for general information about a topic, \"\n",
    "            \"definitions, explanations, or factual details that are likely to be found on websites. \"\n",
    "            \"This is a general-purpose search tool.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "weather_info_tool = FunctionTool.from_defaults(\n",
    "        fn=wit.get_weather_info,\n",
    "        name=\"weather_info_tool\",\n",
    "        description=\"Use this tool ONLY when the user is asking for weather information for a specific location.\"\n",
    "    )\n",
    "\n",
    "hub_stats_tool = FunctionTool.from_defaults(\n",
    "        fn=hst.get_hub_stats,\n",
    "        name=\"hub_stats_tool\",\n",
    "        description=(\n",
    "            \"Use this tool to find statistics about models on the Hugging Face Hub. \"\n",
    "            \"Specifically, use this tool to find the most downloaded model for a given author or organization on the Hugging Face Hub.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Create Alfred with all the tools\n",
    "alfred = ReActAgent.from_tools(\n",
    "        [search_tool, weather_info_tool, hub_stats_tool],\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "        system_prompt=(\n",
    "                \"You are a helpful assistant that can answer questions by using the provided tools. \"\n",
    "                \"When asked a question, first create a plan to answer the question by breaking it down into smaller steps and identifying which tools are needed for each step. \"\n",
    "                \"After creating the plan, execute the steps using the tools. \"\n",
    "                \"Once you have gathered all the necessary information from the tools, your next thought should be to synthesize the information. \"\n",
    "                \"Then, synthesize all the information from the tool observations into a comprehensive and coherent answer that addresses all parts of the user's original question. \"\n",
    "                \"Always follow the thought-action-input format for tool use.\"\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ© Alfred's Response:\n",
      "Here's a comprehensive answer to the user's question:\n",
      "\n",
      "Facebook is a social media platform that allows users to connect with friends, family, and communities of people who share similar interests. The Facebook app features various tools that make it easy to discover new connections, such as Groups, Watch, and Marketplace.\n",
      "\n",
      "As for their most popular model, according to Hugging Face, the leading author associated with Facebook models, the most downloaded model is indeed \"facebook/esmfold_v1\" - a remarkable 20,947,347 downloads! This suggests that this particular model has gained significant traction among users.\n"
     ]
    }
   ],
   "source": [
    "# In your main execution:\n",
    "question = \"What is Facebook and what's their most popular model?\"\n",
    "\n",
    "tools_dict = {\n",
    "     \"dd_search_tool\": search_tool,\n",
    "     \"hub_stats_tool\": hub_stats_tool,\n",
    "     \"weather_info_tool\": weather_info_tool\n",
    " }\n",
    "\n",
    "response_data = await generalized_multi_step_agent(question, llm, tools_dict)\n",
    "\n",
    "print_details = False # Set to False to print only the final answer\n",
    "\n",
    "if print_details:\n",
    "    print(\"--- Execution Details ---\")\n",
    "    for task, result in response_data[\"tool_results\"].items():\n",
    "        print(f\"Executed '{task}': {result}\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "print(\"ðŸŽ© Alfred's Response:\")\n",
    "print(response_data[\"final_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
